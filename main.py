from fastapi import FastAPI
import pickle
import numpy as np
from pydantic import BaseModel
import traceback

app = FastAPI()

# ‚úÖ Load the trained Random Forest model
try:
    with open("rfc.pkl", "rb") as model_file:
        model = pickle.load(model_file)
    print("‚úÖ Random Forest Model Loaded Successfully!")
except Exception as e:
    print(f"‚ùå Error Loading Model: {e}\n{traceback.format_exc()}")

# ‚úÖ Load the label encoder
try:
    with open("label_encoder.pkl", "rb") as encoder_file:
        label_encoder = pickle.load(encoder_file)
    print("‚úÖ Label Encoder Loaded Successfully!")
except Exception as e:
    print(f"‚ùå Error Loading Label Encoder: {e}\n{traceback.format_exc()}")

# ‚úÖ Load the scaler (NEW)
try:
    with open("scaler.pkl", "rb") as scaler_file:
        scaler = pickle.load(scaler_file)
    print("‚úÖ Scaler Loaded Successfully!")
except Exception as e:
    print(f"‚ùå Error Loading Scaler: {e}\n{traceback.format_exc()}")

# ‚úÖ Define Input Model
class AveragesData(BaseModel):
    nitrogen: float
    phosphorus: float
    potassium: float
    temperature: float
    humidity: float
    rainfall: float
    soilPH: float

# ‚úÖ Global variable to store the latest data and recommendation
latest_data = None
latest_recommendation = None

@app.get("/")
def read_root():
    """ Root endpoint to check API status """
    if latest_data and latest_recommendation:
        return {
            "message": "Crop Recommendation API is Running!",
            "latest_data": latest_data,
            "latest_recommendation": latest_recommendation,
        }
    else:
        return {"message": "Crop Recommendation API is Running! No data received yet."}

@app.post("/receive-averages/")
def receive_averages(data: AveragesData):
    """ Receive average data and predict top 3 crop recommendations """
    global latest_data, latest_recommendation

    try:
        print("‚úÖ Received Average Data:", data)

        # Convert input to NumPy array
        input_data = np.array([[ 
            data.nitrogen, data.phosphorus, data.potassium,
            data.temperature, data.humidity, data.soilPH, data.rainfall
        ]])

        print(f"üü° Raw Input Data: {input_data}")

        # ‚úÖ Scale the input data (IMPORTANT FIX)
        scaled_input = scaler.transform(input_data)
        print(f"üü¢ Scaled Input Data: {scaled_input}")

        # ‚úÖ Predict probabilities for each class (crop)
        probas = model.predict_proba(scaled_input)[0]
        print(f"üü† Predicted Probabilities: {probas}")

        # Get the top 3 crop indices (highest probabilities)
        top_indices = np.argsort(probas)[-3:][::-1]  # Sort in descending order
        top_crops = []

        # Map the indices to crop names and confidence scores
        for idx in top_indices:
            crop = label_encoder.inverse_transform([idx])[0]
            confidence = round(probas[idx] * 100, 2)
            top_crops.append({"crop": crop, "confidence": confidence})

        print(f"‚úÖ Top 3 Recommended Crops: {top_crops}")

        # Store the received data and recommendation
        latest_data = data.dict()
        latest_recommendation = top_crops[0]["crop"]  # The best crop

        # Return the top 3 recommended crops
        return {"top_3_recommended_crops": top_crops}

    except Exception as e:
        print(f"‚ùå Error Receiving Averages: {e}\n{traceback.format_exc()}")
        return {"error": f"Failed to process averages: {e}"}


from fastapi.middleware.cors import CORSMiddleware

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allow all domains (replace with specific domains if needed)
    allow_credentials=True,
    allow_methods=["*"],  # Allow all HTTP methods (GET, POST, etc.)
    allow_headers=["*"],  # Allow all headers
)
